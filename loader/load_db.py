import os

from langchain_openai import OpenAIEmbeddings

from config.config import OPENAI_API_KEY, QDRANT_SERVER, QDRANT_API_KEY, collection_name
from qdrant_client import models

from langchain_qdrant import Qdrant

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
# text-embedding-ada-002: T·∫°o ra c√°c vector embedding v·ªõi 1536 chi·ªÅu.
# text-embedding-3-large: T·∫°o ra c√°c vector v·ªõi 3072 chi·ªÅu.


# load db
def load_vector_db():
    try:
        # Initialize the Qdrant client with embeddings for the collection
        client = Qdrant.from_existing_collection(
            embedding=embeddings,  # embeddings are set up in the Qdrant client
            collection_name=collection_name,
            url=QDRANT_SERVER,
            api_key=QDRANT_API_KEY,
        )
        print("Load db th√†nh c√¥ng")
        return client
    except Exception as e:
        print(f"L·ªói load db: {e}")
        return None


# search t∆∞∆°ng t·ª±
def similarity_search_qdrant_data(db, query, k=10):
    # Perform similarity search without passing embeddings explicitly
    docs = db.similarity_search(query=query, k=k)
    return docs


def get_full_page_content(db, title: str, page: int):
    """L·∫•y to√†n b·ªô n·ªôi dung t·ª´ 1 trang c·ª• th·ªÉ c·ªßa t√†i li·ªáu ch·ªâ ƒë·ªãnh"""

    # Th√™m filter theo title ƒë·ªÉ tr√°nh tr√πng l·∫∑p gi·ªØa c√°c t√†i li·ªáu
    filter_conditions = [
        models.FieldCondition(
            key="metadata.page",
            match=models.MatchValue(value=page),
        ),
        models.FieldCondition(
            key="metadata.source",
            match=models.MatchValue(value=title),
        )
    ]

    # Th√™m limit h·ª£p l√Ω v√† score threshold
    docs = db.similarity_search(
        query="",
        filter=models.Filter(must=filter_conditions),
    )
    return docs


def get_context_db(db, query, k=10):
    docs = similarity_search_qdrant_data(db, query, k)

    # T·∫°o dict l∆∞u tr·ªØ c√°c trang duy nh·∫•t v·ªõi metadata ƒë·∫ßy ƒë·ªß
    unique_pages = {}
    for doc in docs:
        meta = doc.metadata
        title = meta.get('source').strip()
        page = int(meta.get('page', -2))
        unique_key = f"{title}_{page}"
        if unique_key not in unique_pages:
            unique_pages[unique_key] = {
                'source': title,
                'page': page
            }

    all_docs = []  # Ch·ªó n√†y ƒë·ªïi t√™n bi·∫øn ƒë·ªÉ tr√°nh tr√πng l·∫∑p v·ªõi docs
    for key, data in unique_pages.items():
        # N·∫øu get_full_page_content tr·∫£ v·ªÅ m·ªôt danh s√°ch, b·∫°n c·∫ßn x·ª≠ l√Ω t·ª´ng ph·∫ßn t·ª≠ trong ƒë√≥
        page_docs = get_full_page_content(db, data['source'], data['page'])
        if isinstance(page_docs, list):  # N·∫øu l√† list, th√™m t·∫•t c·∫£ v√†o all_docs
            all_docs.extend(page_docs)
        else:  # N·∫øu kh√¥ng ph·∫£i list, ch·ªâ th√™m 1 t√†i li·ªáu v√†o all_docs
            all_docs.append(page_docs)

    # S·∫Øp x·∫øp v√† lo·∫°i b·ªè tr√πng l·∫∑p theo chunk_id
    unique_id = set()
    sorted_docs = sorted(all_docs, key=lambda x: x.metadata.get('page', 0))
    unique_docs = []
    for doc in sorted_docs:
        chunk_id = doc.metadata.get('chunk_id')  # L·∫•y chunk_id t·ª´ metadata
        if chunk_id not in unique_id:
            unique_id.add(chunk_id)
            unique_docs.append(doc)
    # print(len(sorted_docs))
    # print(len(unique_docs))
    context_items = []
    for doc in unique_docs:
        data = doc.metadata
        content = doc.page_content
        context_items.append(
            f"üìñ {data['source'].upper()} - TRANG {data['page'] + 1}\n"
            f"üîç N·ªôi dung:\n{content}\n"
            "‚Äï--------------------------------------"
        )
        print(doc.metadata['chunk_id'])
    print("\n\n".join(context_items))
    return "\n\n".join(context_items)

# Ki·ªÉm tra v·ªõi c√¢u truy v·∫•n
get_context_db(load_vector_db(), "Chuy√™n l√† g√¨ ? ")


# Example usage:
# save_vector_db(load_document("../data/upload/So tay Van hoa Doanh nghiep - Stavian Group.pdf"))

# client = load_vector_db()
# query = "T√¥n gi√°o trong truy·∫øt h·ªçc"
# docs = similarity_search_qdrant_data(client, query)
# for doc in docs:
#     print(doc.page_content, "\n", doc.metadata.get('package'))
